{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept1. Tokenization with nltk\n",
    "\n",
    "**Splitting a string into a list of words is known as tokenization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import wordpunct_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi,', 'how', 'are', 'you', 'doing', 'today?']\n"
     ]
    }
   ],
   "source": [
    "s = \"hi, how are you doing today?\"\n",
    "\n",
    "word_split = s.split()\n",
    "print(word_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "# The split method of the string doesn't split the punctuations <comma and question mark> which doesn't \n",
    "# have much impact in understandindg the context of the sentenence. So, we're using nltk work_tokenize.\n",
    "\n",
    "s = \"hi, how are you doing today?\"\n",
    "word_tokens = word_tokenize(s)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "word_tokens_punc = wordpunct_tokenize(s)\n",
    "print(word_tokens_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept1b. Tokenization & word-indicing using tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzier API\n",
    "\n",
    "**Tokenizer can be fit on raw text or integer encoded text documents.**\n",
    "\n",
    "**Once fit, the Tokenizer provides 4 attributes that we can use to learned about your documents:**\n",
    "\\\n",
    " word counts: A dictionary of words and their counts.\n",
    "\\\n",
    " word docs: An integer count of the total number of documents that were used to fit the\n",
    "Tokenizer.\n",
    "\\\n",
    " word index: A dictionary of words and their uniquely assigned integers.\n",
    "\\\n",
    " document count: A dictionary of words and how many documents each appeared in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four attributes:\n",
      "\n",
      "word counts: OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "\n",
      "word docs: defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
      "\n",
      "word index: {'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "\n",
      "document count: 5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "print(\"Four attributes:\")\n",
    "\n",
    "# A dictionary of words and their counts.\n",
    "print(f\"\\nword counts: {tokenizer.word_counts}\")\n",
    "\n",
    "# An integer count of the total number of documents that were used to fit the Tokenizer.\n",
    "print(f\"\\nword docs: {tokenizer.word_docs}\")\n",
    "\n",
    "# A dictionary of words and their uniquely assigned integers.\n",
    "print(f\"\\nword index: {tokenizer.word_index}\")\n",
    "\n",
    "# A dictionary of words and how many documents each appeared in.\n",
    "print(f\"\\ndocument count: {tokenizer.document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "{'you': 1, 'hello': 2, 'how': 3, 'are': 4, 'im': 5, 'getting': 6, 'bored': 7, 'at': 8, 'home': 9, 'and': 10, 'what': 11, 'do': 12, 'think': 13, 'did': 14, 'know': 15, 'about': 16, 'counts': 17, \"let's\": 18, 'see': 19, 'if': 20, 'this': 21, 'works': 22, 'yes': 23}\n",
      "\n",
      "Corpus_sequences assigning an integer to each token: [[2, 3, 4, 1], [5, 6, 7, 8, 9, 10, 1, 11, 12, 1, 13], [14, 1, 15, 16, 17], [18, 19, 20, 21, 22], [23]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "corpus_sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(len(tokenizer.word_index))\n",
    "print(tokenizer.word_index)\n",
    "print(\"\\nCorpus_sequences assigning an integer to each token:\", corpus_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_to_word_sequence & one_hot\n",
    "\n",
    "**Keras provides the text to word sequence() function that you can use to split text into a list of words. By\n",
    "default, this function automatically does 3 things:**\n",
    "\n",
    " Splits words by space.\n",
    "\\\n",
    " Filters out punctuation.\n",
    "\\\n",
    " Converts text to lowercase (lower=True)\n",
    "\n",
    "**Also, the integer associated with thw words get changes everytime clear the memory, bcz of stochastic nature of neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the sentence: 8\n",
      "[7, 4, 8, 2, 2, 4, 7, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(f\"Unique words in the sentence: {vocab_size}\")\n",
    "\n",
    "## one_hot seems to perform onehot_encodding, but it's generally perform hashing..\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Encoding with hashing trick\n",
    "\n",
    "**It's like one_hot but It also provides more exibility, allowing you to specify\n",
    "the hash function as either hash (the default) or other hash functions such as the built in md5\n",
    "function or your own function.**\n",
    "\n",
    "**Unlike One_hot, with the use of a different hash function results in consistent, but different integers\n",
    "for words as the one hot() function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the sentence: 8\n",
      "[10, 5, 8, 9, 10, 8, 10, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing.text import hashing_trick\n",
    "data = 'The quick brown fox jumped over the lazy dog.'\n",
    "tokens = set(text.text_to_word_sequence(data))\n",
    "vocab_size = len(tokens)\n",
    "\n",
    "print(f\"Unique words in the sentence: {vocab_size}\")\n",
    "results = hashing_trick(data, round(vocab_size*1.4), hash_function=\"md5\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept2. Bag of words\n",
    "\n",
    "**In bag of words, we create a huge sparse matrix that stores counts of all the words in our corpus (corpus = all the documents = all the sentences).**\n",
    "\n",
    "**For this, we will use CountVectorizer from scikit-learn.**\n",
    "The way CountVectorizer works is it first tokenizes the sentence and then assigns a\n",
    "value to each token. So, each token is represented by a unique index. These unique\n",
    "indices are the columns that we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 9, 'how': 11, 'are': 2, 'you': 22, 'im': 13, 'getting': 8, 'bored': 4, 'at': 3, 'home': 10, 'and': 1, 'what': 19, 'do': 7, 'think': 17, 'did': 6, 'know': 14, 'about': 0, 'counts': 5, 'let': 15, 'see': 16, 'if': 12, 'this': 18, 'works': 20, 'yes': 21}\n",
      "\n",
      " Stopwords---- set()\n",
      "Features i.e. words from a given sentences\n",
      "['about', 'and', 'are', 'at', 'bored', 'counts', 'did', 'do', 'getting', 'hello', 'home', 'how', 'if', 'im', 'know', 'let', 'see', 'think', 'this', 'what', 'works', 'yes', 'you']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "\n",
    "ctv = CountVectorizer()\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed = ctv.transform(corpus)  ## <class 'scipy.sparse.csr.csr_matrix'> i.e. sparse_matrix\n",
    "print(ctv.vocabulary_)\n",
    "print(\"\\n Stopwords----\", ctv.stop_words_)\n",
    "\n",
    "print(\"Features i.e. words from a given sentences\")\n",
    "print(ctv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The first sentence denoted by 0 and each word with a number as mentioned in vocabulary and its count.\n",
    "## e.g. (0,2) -- 0 means 1st sentence & 2 is indx of are as mentioned in vocabulary.\n",
    "## We see that index 22 belongs to “you” and in the second sentence, we have used\n",
    "## “you” twice. Thus, the count is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 2)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 22)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 22)\t2\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 22)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 15)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 20)\t1\n",
      "  (4, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_transformed))\n",
    "print(corpus_transformed)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Above, special characters were missing. Let’s integrate word_tokenize from scikit-learn in CountVectorizer and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "\n",
    "ctv = CountVectorizer(tokenizer = word_tokenize)\n",
    "ctv.fit(corpus)\n",
    "\n",
    "corpus_transformed_wt = ctv.transform(corpus)\n",
    "\n",
    "# This changes our vocabulary, now puncutations are also included :\n",
    "print(ctv.vocabulary_)\n",
    "\n",
    "print(ctv.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 27)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 27)\t2\n",
      "  (2, 5)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 27)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 20)\t1\n",
      "  (3, 21)\t1\n",
      "  (3, 23)\t1\n",
      "  (3, 25)\t1\n",
      "  (4, 0)\t4\n",
      "  (4, 26)\t1\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept3. TF-IDF\n",
    "\n",
    "**here we get float for each word, whereas in CountVectorizer getting count of each word in a sentence.\n",
    "The drawback of CountVectorizer is:\n",
    "    different words have same count, may have different index but have same count.\n",
    "    word having greater count have more influence thus the approach is not that good**\n",
    "\n",
    "**TF-IDF represent count by float so better than CountVectorizer, still this appproach there is some influence of the larger number.**\n",
    "\n",
    "\n",
    "#### State of Art approach is ---->> Word embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(token_pattern=None,\n",
      "                tokenizer=<function word_tokenize at 0x0000023E72C53EA0>)\n",
      "{'hello': 14, ',': 2, 'how': 16, 'are': 7, 'you': 27, '?': 4, 'im': 18, 'getting': 13, 'bored': 9, 'at': 8, 'home': 15, '.': 3, 'and': 6, 'what': 24, 'do': 12, 'think': 22, 'did': 11, 'know': 19, 'about': 5, 'counts': 10, 'let': 20, \"'s\": 1, 'see': 21, 'if': 17, 'this': 23, 'works': 25, '!': 0, 'yes': 26}\n",
      "\n",
      "Features names:--\n",
      "['!', \"'s\", ',', '.', '?', 'about', 'and', 'are', 'at', 'bored', 'counts', 'did', 'do', 'getting', 'hello', 'home', 'how', 'if', 'im', 'know', 'let', 'see', 'think', 'this', 'what', 'works', 'yes', 'you']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a corpus of sentences\n",
    "corpus = [\n",
    "\"hello, how are you?\",\n",
    "\"im getting bored at home. And you? What do you think?\",\n",
    "\"did you know about counts\",\n",
    "\"let's see if this works!\",\n",
    "\"YES!!!!\"\n",
    "]\n",
    "\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "print(tfv)\n",
    "tfv.fit(corpus)\n",
    "\n",
    "corpus_transformed= tfv.transform(corpus)\n",
    "\n",
    "print(tfv.vocabulary_)\n",
    "print(\"\\nFeatures names:--\")\n",
    "print(tfv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: ***We can see that instead of integer values, this time we get floats.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 27)\t0.2965698850220162\n",
      "  (0, 16)\t0.4428321995085722\n",
      "  (0, 14)\t0.4428321995085722\n",
      "  (0, 7)\t0.4428321995085722\n",
      "  (0, 4)\t0.35727423026525224\n",
      "  (0, 2)\t0.4428321995085722\n",
      "  (1, 27)\t0.35299699146792735\n",
      "  (1, 24)\t0.2635440111190765\n",
      "  (1, 22)\t0.2635440111190765\n",
      "  (1, 18)\t0.2635440111190765\n",
      "  (1, 15)\t0.2635440111190765\n",
      "  (1, 13)\t0.2635440111190765\n",
      "  (1, 12)\t0.2635440111190765\n",
      "  (1, 9)\t0.2635440111190765\n",
      "  (1, 8)\t0.2635440111190765\n",
      "  (1, 6)\t0.2635440111190765\n",
      "  (1, 4)\t0.42525129752567803\n",
      "  (1, 3)\t0.2635440111190765\n",
      "  (2, 27)\t0.31752680284846835\n",
      "  (2, 19)\t0.4741246485558491\n",
      "  (2, 11)\t0.4741246485558491\n",
      "  (2, 10)\t0.4741246485558491\n",
      "  (2, 5)\t0.4741246485558491\n",
      "  (3, 25)\t0.38775666010579296\n",
      "  (3, 23)\t0.38775666010579296\n",
      "  (3, 21)\t0.38775666010579296\n",
      "  (3, 20)\t0.38775666010579296\n",
      "  (3, 17)\t0.38775666010579296\n",
      "  (3, 1)\t0.38775666010579296\n",
      "  (3, 0)\t0.3128396318588854\n",
      "  (4, 26)\t0.2959842226518677\n",
      "  (4, 0)\t0.9551928286692534\n"
     ]
    }
   ],
   "source": [
    "print(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept4. n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokens--- ['Hello', ',', 'how', 'are', 'you', '?']\n",
      "\n",
      "n_grams for the given sentence with N=2 combinations:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', ',', 'how'),\n",
       " (',', 'how', 'are'),\n",
       " ('how', 'are', 'you'),\n",
       " ('are', 'you', '?')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=3\n",
    "sentence = \"Hello, how are you?\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(f\"word_tokens--- {tokens}\")\n",
    "\n",
    "n_grams = list(ngrams(tokens, N))\n",
    "print()\n",
    "print(\"n_grams for the given sentence with N=2 combinations:\")\n",
    "n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "**Both CountVectorizer and TfidfVectorizer implementations of scikit-learn offers ngrams\n",
    "by ngram_range parameter, which has a minimum and maximum limit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"../input/IMDB_Dataset-folds.csv\", nrows=10000)\n",
    "corpus = corpus.review.values\n",
    "len(corpus)\n",
    "\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "\n",
    "corpus_transformed = tfv.fit_transform(corpus)\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "\n",
    "corpus_svd = svd.fit(corpus_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>\n",
      "TruncatedSVD(n_components=10)\n"
     ]
    }
   ],
   "source": [
    "print(type(svd))\n",
    "print(svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.decomposition._truncated_svd.TruncatedSVD"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x08\\x08\\x08\\x08a', '!', '#', '$', '%', '&', \"'\", \"''\", \"''and\", \"''the\", \"'00s\", \"'01\", \"'03\", \"'04\", \"'05\", \"'06\", \"'07\", \"'08\", \"'10\", \"'10.5\", \"'12\", \"'15\", \"'20\", \"'20th\", \"'24\", \"'28\", \"'30\", \"'30s\", \"'30s-early\", \"'30s/'40s\", \"'32\", \"'34\", \"'39\", \"'40\", \"'40s\", \"'42\", \"'43\", \"'46\", \"'48\", \"'50\", \"'50s\", \"'51\", \"'53\", \"'54\", \"'55\", \"'56\", \"'59\", \"'60\", \"'60s\", \"'60s.\", \"'60´s\", \"'62\", \"'64\", \"'66\", \"'70\", \"'70's-style\", \"'70's.\", \"'70s\", \"'71\", \"'73\", \"'77\", \"'79\", \"'80\", \"'80s\", \"'80s/early\", \"'81\", \"'84\", \"'86\", \"'87\", \"'88\", \"'90\", \"'90s\", \"'92\", \"'93\", \"'94-'95\", \"'95\", \"'96\", \"'97\", \"'99\", \"'aaaaagh\", \"'aasmaan\", \"'about\", \"'absorbed\", \"'ace\", \"'ack\", \"'act\", \"'acting\", \"'action\", \"'actor\", \"'actors\", \"'actual\", \"'addiction\"]\n",
      "<class 'method'>\n"
     ]
    }
   ],
   "source": [
    "## get the words present inthe senetence..\n",
    "print(tfv.get_feature_names()[0:92])\n",
    "print(type(tfv.get_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69838"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 69838)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "\n",
    "feature_scores = dict(zip(\n",
    "                        tfv.get_feature_names(), \n",
    "                        corpus_svd.components_[sample_index]\n",
    "                        )\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
